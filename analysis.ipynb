{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workbook Intention\n",
    "\n",
    "This workbook is designed to demonstrate and end-to-end model development workflow, leveraging scalable and efficient coding and infrastructure practices based on our internal Xometry ML Platform capabilities.\n",
    "\n",
    "**It should be possible to complete this workflow without the need to create a sagemaker project or any other kind of governed, persistent infrastructure.** In this way, these practices translate to one-off POC type development work as well as work that needs to result in a deployable model.\n",
    "\n",
    "The expected steps are as follows:\n",
    "\n",
    "1. Read data from a database\n",
    "1. Split data into train/test/validation sets\n",
    "1. Conduct feature engineering\n",
    "1. Conduct HPO for multiple algorithms \n",
    "1. Train a model based on the best HPO run\n",
    "1. Conduct inference on new data using the trained model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#library imports \n",
    "import boto3\n",
    "import os\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "import xoml_sagemaker.pipeline_types\n",
    "import xoml_sagemaker.generate_data\n",
    "import xoml_sagemaker.processing\n",
    "import xoml_sagemaker.feature_engineering\n",
    "from xoml_sagemaker.pipeline_types import StaticHyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your project name.\n",
    "# This will be prepended to remote job names for easier identification when debugging.\n",
    "project_name = 'train-updt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. Read data from database\n",
    "\n",
    "**GOAL:**\n",
    "\n",
    "Create a dataset on which to train a model.\n",
    "\n",
    "**TODO:**\n",
    " - support feature store queries\n",
    " - expose instance_type parameter on processing job, as our method of moving data from source to s3 data brings it into memory, we will at some point encounter the need for this to be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = xoml_sagemaker.pipeline_types.GenerateData(\n",
    "    JobName=project_name,\n",
    "    JobType=\"Snowflake\",\n",
    "    SQLFile=\"./1. sql/query.sql\",\n",
    ")\n",
    "data_res = xoml_sagemaker.generate_data.launch_generate_data_job(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2. Split data\n",
    "**GOAL:**\n",
    "\n",
    "Split the result of Step 1 according to the instructions in `2. split_data/split_data.py`.\n",
    "\n",
    "**TODO:**\n",
    "\n",
    " - demo support for args (e.g., for train/test and train/test/validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_config = xoml_sagemaker.pipeline_types.Preprocess(\n",
    "    JobType=\"Managed\",\n",
    "    JobName=project_name,\n",
    "    Framework=\"SKLearn\",\n",
    "    FrameworkVersion=\"0.20.0\",\n",
    "    Code=\"split_data.py\",\n",
    "    CodeSourceDir=\"./2. split_data/\",\n",
    "    InputS3Dir=data_res.output_path\n",
    ")\n",
    "\n",
    "split_res = xoml_sagemaker.processing.launch_preprocessing_job(split_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "\n",
    "**GOAL:**\n",
    "Conduct feature engineering on each of the outputs of the `split_data` step, as instructed in `3. feature_engineering/ft_eng.py`\n",
    "\n",
    "**TODO:**\n",
    "\n",
    " - update the example to not take paths for output or python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering_config = xoml_sagemaker.pipeline_types.FeatureEngineering(\n",
    "    job_name=project_name,\n",
    "    job_type=\"Managed\",\n",
    "    # required params for a managed job\n",
    "    framework=\"SKLearn\",\n",
    "    framework_version=\"0.23-1\",\n",
    "    code_source_dir=\"./3. feature_engineering/\",\n",
    "    code=\"ft_eng.py\",\n",
    "    python_version=\"py3\",\n",
    "    # params for input and output\n",
    "    input_s3_dir=\"s3://data-science-826190527795-lizleki/train-updt-45846674-pwgb/output/result/\", #split_res.output_path,\n",
    "    model_s3_dir=\"s3://data-science-826190527795-lizleki/train-updt/\",\n",
    "    transformed_data_s3_dir=\"s3://data-science-826190527795-lizleki/train-updt\",\n",
    ")\n",
    "    \n",
    "ft_eng_res = xoml_sagemaker.feature_engineering.launch_feature_engineering_job(feature_engineering_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper Parameter Optimization & Experiment Tracking\n",
    "\n",
    "At this stage, we want to run an HPO job to build many models and compare their performance on some objective metric. We also want to track each of those runs in our mlflow server.\n",
    "\n",
    "functional expectations \n",
    " - hpo job is capable of comparing results across one or more estimators (e.g., xgboost vs. pytorch), with an hpo grid for each.\n",
    "- exposure of relevant job configuration via arguments with logical default values\n",
    " - every run of the hpo job is tracked to a single experiment \n",
    " - enforcement of a standard job name\n",
    " - enforcement of a standard experiment name\n",
    "   - also the ability to override the experiment name, to allow the DS to run this step many times and log to the same experiment.\n",
    " - enforecement of a standard on destination of the outputs\n",
    "    - something like user_bucket/hpo/jobname/train.csv, user_bucket/hpo/jobname/test.csv, user_bucket/hpo/jobname/val.csv\n",
    "\n",
    "other thoughts \n",
    " - this will be the trickiest to abstract, in my opinion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Final Model\n",
    "\n",
    "At this stage, we understand what algorithm and hyperparameter specification is the most performant. We now want to write a script to train that model, so that we it can be submitted to our pipeline for registration and deployment. \n",
    "\n",
    "The goal here is simply to test that our `train.py` script does result in the model we expect.\n",
    "\n",
    "functional expectations \n",
    " - enforcement of a standard job name\n",
    "- exposure of relevant job configuration via arguments with logical default values\n",
    "\n",
    " - support for custom docker, if needed (standard will be to use managed images)\n",
    " - enforecement of a standard on destination of the outputs\n",
    "    - something like user_bucket/train/jobname/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_config = xoml_sagemaker.pipeline_types.Train(\n",
    "    job_type=\"Managed\",\n",
    "    job_name=project_name,\n",
    "    framework=\"XGBoost\",\n",
    "    framework_version=\"1.7-1\",\n",
    "    code_source_dir=\"4. train/train.py\",\n",
    "    code=\"train.py\",\n",
    "    python_version=\"py3\",\n",
    "    input_data_dir=\"s3://data-science-826190527795-lizleki/train-updt\",\n",
    "    output_data_dir=\"s3://data-science-826190527795-lizleki/train-updt/{project}/output\".format(project=project_name),\n",
    "    static_hyperparameters=[\n",
    "        StaticHyperparameter(Key=\"num_round\", Value=\"50\"),\n",
    "        StaticHyperparameter(Key=\"max_depth\", value=\"5\"),\n",
    "        StaticHyperparameter(Key=\"eta\", Value=\"0.2\"),\n",
    "        StaticHyperparameter(Key=\"objective\", Value=\"reg:squarederror\"),\n",
    "        StaticHyperparameter(Key=\"gamma\", Value=\"4\"),\n",
    "    ],\n",
    ")\n",
    "launched_job = train_module.launch_training_job(train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A LTERNATIVE TO TRAIN FINAL MODEL **\n",
    "#### Generate the config.yaml\n",
    "\n",
    "We know that some DS teams create and ensemble model based on the top N HPO runs (as an example see [CNC Cost Model](https://github.com/xometry/datasci-cnc-cost-model/tree/master)). For these types of models, it does not make sense to run the `train_model` step. Instead, we would need a function to handle the ensembling workflow, so the data scientist could assess the ensembled model.\n",
    "\n",
    "This is probably a processing job. We should use the same code used by the legacy infrastructure. Let's avoid changes to ensure that we don't introduce an unexpected outcome.\n",
    "\n",
    "functional expectations \n",
    "- takes as input the number of models to ensemble\n",
    "- has logical defaults that can be overridden \n",
    "    - e.g., the name of the HPO job to reference (we can default to the last run by them, if we're working on an object)\n",
    "- enforecement of a standard on destination of the outputs\n",
    "    - something like user_bucket/ensemble/jobname/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_desc = workflow.ensemble_model(n_jobs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(process_desc['test_data'])\n",
    "endpoint_desc.predict(df.loc(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTIONAL**\n",
    "#### Generate the config.yaml\n",
    "\n",
    "Our model training and registration pipeline expects that configurations be passed via a yaml file. It will be necessary for data scientists to populate this yaml with a predefined structure and parameter values which have been 'discovered' during this interactive workflow. It has been proposed that we generate the yaml for them. This may solve that\n",
    "\n",
    "functional expectations \n",
    "- values based on the latest runs of the processing, training, and endpoint generation jobs\n",
    "- ability to override those values, if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.update_config(destination=\"./config.yaml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
